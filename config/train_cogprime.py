# nanoGPT Configuration for Training on CogPrime Corpus
# Based on train_shakespeare_char.py and train_gpt2.py, adapted for a custom corpus
# containing CogPrime documentation and OpenCog Scheme code.

# --- Output and Logging ---
out_dir = 'out-cogprime'
eval_interval = 500 # Evaluate less frequently than very small datasets
eval_iters = 200    # Number of iterations for validation loss estimation
log_interval = 10   # Print training loss to console

# Save checkpoints periodically by iteration, as training might be long
always_save_checkpoint = True

# WandB logging (optional, can be overridden via command line)
wandb_log = False
wandb_project = 'cogprime'
wandb_run_name = 'nanogpt-cogprime' # e.g., 'run' + str(time.time())

# --- Data ---
dataset = 'cogprime' # This should match the directory name in data/
                     # where train.bin and val.bin will be generated by prepare.py

# Training hyperparameters
# Effective batch size will be batch_size * gradient_accumulation_steps
# E.g., 16 * 4 = 64
batch_size = 16 # Micro-batch size, adjust based on GPU memory
gradient_accumulation_steps = 4

block_size = 768 # Context length: increased for technical documents and code
                 # Max is 1024 for standard GPT-2 BPE tokenizer if memory allows,
                 # but 768 is a good compromise.

# --- Model ---
# A model slightly larger than the Shakespeare example, but smaller than GPT-2 124M
# Adjust based on available compute and desired model capacity.
n_layer = 8     # Number of transformer layers
n_head = 8      # Number of attention heads
n_embd = 512    # Embedding dimension
dropout = 0.1   # Dropout rate for regularization (0.0 for no dropout)
bias = True     # Whether to use bias in Linear and LayerNorm layers.
                # GPT-2 pretraining often sets bias=False in LayerNorms,
                # but for a smaller model trained from scratch, True is fine.

# --- Optimizer (AdamW) ---
learning_rate = 3e-4    # Max learning rate (a common starting point for transformers)
max_iters = 20000       # Total number of training iterations (adjust based on dataset size and convergence)
weight_decay = 1e-1     # Weight decay for regularization
beta1 = 0.9
beta2 = 0.95            # More stable for AdamW than 0.99 or 0.999 for larger models/datasets
grad_clip = 1.0         # Clip gradients at this value (0.0 to disable)

# --- Learning Rate Decay ---
decay_lr = True         # Whether to decay the learning rate
warmup_iters = 200      # Number of warmup iterations
lr_decay_iters = 20000  # Should generally be equal to max_iters
min_lr = 3e-5           # Minimum learning rate (learning_rate / 10)

# --- System ---
device = 'cuda'         # 'cpu', 'cuda', 'cuda:0', 'cuda:1', 'mps' (for Apple Silicon)
dtype = 'float16'       # 'float32', 'bfloat16', or 'float16'.
                        # 'float16' uses automatic mixed precision and GradScaler.
                        # 'bfloat16' is generally preferred if available (e.g., A100+ GPUs) for stability.
compile = True          # Use PyTorch 2.0 to compile the model for speed
                        # Set to False if encountering issues or on platforms without support.
