# nanoGPT Configuration for Training on CogPrime Corpus
# Based on train_shakespeare_char.py and train_gpt2.py, adapted for a custom corpus
# containing CogPrime documentation and OpenCog Scheme code.

# --- Output and Logging ---
out_dir = 'out-cogprime'
eval_interval = 500 # Evaluate less frequently than very small datasets
eval_iters = 200    # Number of iterations for validation loss estimation
log_interval = 10   # Print training loss to console

# Save checkpoints periodically by iteration, as training might be long
always_save_checkpoint = True

# WandB logging (optional, can be overridden via command line)
wandb_log = False
wandb_project = 'cogprime'
wandb_run_name = 'nanogpt-cogprime' # e.g., 'run' + str(time.time())

# --- Data ---
dataset = 'cogprime' # This should match the directory name in data/
                     # where train.bin and val.bin will be generated by prepare.py

# Training hyperparameters
# Effective batch size will be batch_size * gradient_accumulation_steps
# E.g., 4 * 8 = 32 (smaller for CPU training)
batch_size = 4  # Smaller micro-batch size for CPU training
gradient_accumulation_steps = 8

block_size = 512 # Context length: reduced for CPU training
                 # Smaller context size to reduce memory usage on CPU

# --- Model ---
# Smaller model suitable for CPU training
# Reduced from the original to be more manageable on CPU
n_layer = 6     # Number of transformer layers (reduced for CPU)
n_head = 6      # Number of attention heads (reduced for CPU)
n_embd = 384    # Embedding dimension (reduced for CPU)
dropout = 0.1   # Dropout rate for regularization (0.0 for no dropout)
bias = True     # Whether to use bias in Linear and LayerNorm layers.
                # GPT-2 pretraining often sets bias=False in LayerNorms,
                # but for a smaller model trained from scratch, True is fine.

# --- Optimizer (AdamW) ---
learning_rate = 3e-4    # Max learning rate (a common starting point for transformers)
max_iters = 10000       # Reduced iterations for CPU training
weight_decay = 1e-1     # Weight decay for regularization
beta1 = 0.9
beta2 = 0.95            # More stable for AdamW than 0.99 or 0.999 for larger models/datasets
grad_clip = 1.0         # Clip gradients at this value (0.0 to disable)

# --- Learning Rate Decay ---
decay_lr = True         # Whether to decay the learning rate
warmup_iters = 100      # Reduced warmup iterations for CPU training
lr_decay_iters = 10000  # Should generally be equal to max_iters
min_lr = 3e-5           # Minimum learning rate (learning_rate / 10)

# --- System ---
device = 'cpu'          # 'cpu', 'cuda', 'cuda:0', 'cuda:1', 'mps' (for Apple Silicon)
dtype = 'float32'       # 'float32', 'bfloat16', or 'float16'.
                        # Using float32 for CPU training as float16 requires CUDA
                        # 'bfloat16' is generally preferred if available (e.g., A100+ GPUs) for stability.
compile = False         # Use PyTorch 2.0 to compile the model for speed
                        # Set to False for CPU training and compatibility
